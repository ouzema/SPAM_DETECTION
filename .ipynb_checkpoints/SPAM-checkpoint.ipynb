{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c14be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 14:19:46 WARN Utils: Your hostname, ouzema-Vivobook-ASUSLaptop-M7400QC-M7400QC resolves to a loopback address: 127.0.1.1; using 20.20.20.234 instead (on interface wlp2s0)\n",
      "23/11/15 14:19:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/15 14:19:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/15 14:19:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/11/15 14:19:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSVToDataFrame\").getOrCreate()\n",
    "csv_file_path = \"spam.csv\"\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"class\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "data = spark.read.option(\"delimiter\", \",\").csv(csv_file_path, schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f7650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 14:19:50 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/SPAM_DETECTION/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if that��s t...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f1574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if that��s t...|    58|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 14:19:51 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/SPAM_DETECTION/spam.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "data = data.withColumn('length',length(data['text']))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225f81fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 14:23:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: v1\n",
      " Schema: class\n",
      "Expected: class but found: v1\n",
      "CSV file: file:///home/ouzema/Big%20Data/SPAM_DETECTION/spam.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4825"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"class\").where(data[\"class\"] == \"ham\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79cbd7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 14:23:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: v1\n",
      " Schema: class\n",
      "Expected: class but found: v1\n",
      "CSV file: file:///home/ouzema/Big%20Data/SPAM_DETECTION/spam.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"class\").where(data[\"class\"] == \"spam\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2294f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "data = data.withColumn(\"class\", when(data[\"class\"] == \"ham\\\"\\\"\\\"\", \"ham\").otherwise(data[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ef1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:13 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|class|       avg(length)|\n",
      "+-----+------------------+\n",
      "|  ham| 71.07065893079155|\n",
      "| spam|138.45917001338688|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee95c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27458149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c615cf",
   "metadata": {},
   "source": [
    "### Traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb8f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: v2\n",
      " Schema: text\n",
      "Expected: text but found: v2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.filter(col(\"text\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1952de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9796aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\"N/A\", subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2812de7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:14 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: v2\n",
      " Schema: text\n",
      "Expected: text but found: v2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: v2\n",
      " Schema: text\n",
      "Expected: text but found: v2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n",
    "cleaner = data_prep_pipe.fit(data)\n",
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18928f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13377,[7,10,31,6...|\n",
      "|  0.0|(13377,[0,23,298,...|\n",
      "|  1.0|(13377,[2,13,19,2...|\n",
      "|  0.0|(13377,[0,68,78,1...|\n",
      "|  0.0|(13377,[35,134,31...|\n",
      "|  1.0|(13377,[11,66,140...|\n",
      "|  0.0|(13377,[11,51,107...|\n",
      "|  0.0|(13377,[126,186,4...|\n",
      "|  1.0|(13377,[1,45,120,...|\n",
      "|  1.0|(13377,[0,1,13,26...|\n",
      "|  0.0|(13377,[18,42,116...|\n",
      "|  1.0|(13377,[8,17,36,8...|\n",
      "|  1.0|(13377,[13,28,45,...|\n",
      "|  0.0|(13377,[38,95,226...|\n",
      "|  0.0|(13377,[550,1773,...|\n",
      "|  1.0|(13377,[28,109,11...|\n",
      "|  0.0|(13377,[80,214,44...|\n",
      "|  0.0|(13377,[0,2,49,13...|\n",
      "|  0.0|(13377,[0,74,104,...|\n",
      "|  1.0|(13377,[4,28,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:18 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    }
   ],
   "source": [
    "clean_data = clean_data.select(['label','features'])\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0375f04",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe785b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:18 WARN DAGScheduler: Broadcasting large task binary with size 1166.4 KiB\n",
      "23/11/15 13:40:18 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:19 WARN DAGScheduler: Broadcasting large task binary with size 1143.9 KiB\n",
      "23/11/15 13:40:19 WARN DAGScheduler: Broadcasting large task binary with size 1371.0 KiB\n",
      "23/11/15 13:40:19 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13377,[0,1,2,13,...|[-627.86436019134...|[0.99999999998812...|       0.0|\n",
      "|  0.0|(13377,[0,1,2,40,...|[-1069.9386596866...|[1.0,8.9099224203...|       0.0|\n",
      "|  0.0|(13377,[0,1,7,8,1...|[-1196.5171174199...|[1.0,3.2650163166...|       0.0|\n",
      "|  0.0|(13377,[0,1,10,30...|[-878.44022025012...|[1.0,2.7287908807...|       0.0|\n",
      "|  0.0|(13377,[0,1,14,18...|[-1359.9030015615...|[1.0,3.5262940466...|       0.0|\n",
      "|  0.0|(13377,[0,1,14,31...|[-217.55833523441...|[1.0,7.7706339815...|       0.0|\n",
      "|  0.0|(13377,[0,1,17,19...|[-807.77857531980...|[1.0,1.2896161732...|       0.0|\n",
      "|  0.0|(13377,[0,1,20,26...|[-968.08264758095...|[1.0,6.1869613664...|       0.0|\n",
      "|  0.0|(13377,[0,1,28,11...|[-598.91038846017...|[1.0,3.1073368663...|       0.0|\n",
      "|  0.0|(13377,[0,1,42,67...|[-615.36874821856...|[0.99995131397475...|       0.0|\n",
      "|  0.0|(13377,[0,1,70,10...|[-679.29782774741...|[1.0,4.7711977009...|       0.0|\n",
      "|  0.0|(13377,[0,1,150,1...|[-250.86055299654...|[0.97390535961727...|       0.0|\n",
      "|  0.0|(13377,[0,1,4294,...|[-127.83416052757...|[0.99997754889223...|       0.0|\n",
      "|  0.0|(13377,[0,2,3,6,9...|[-3412.5327743728...|[1.0,9.2406709014...|       0.0|\n",
      "|  0.0|(13377,[0,2,3,6,9...|[-3412.5327743728...|[1.0,9.2406709014...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,11,...|[-1234.3669870806...|[1.0,1.0588857097...|       0.0|\n",
      "|  0.0|(13377,[0,2,5,8,4...|[-832.34963505152...|[1.0,1.6321267447...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,1...|[-687.00217369232...|[1.0,2.4451643463...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,1...|[-454.11325043857...|[1.0,1.2615567591...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,10,...|[-738.01244433038...|[1.0,5.9367818148...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:19 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "(training,testing) = clean_data.randomSplit([0.7,0.3])\n",
    "spam_predictor = nb.fit(training)\n",
    "test_results = spam_predictor.transform(testing)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e903f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:20 WARN DAGScheduler: Broadcasting large task binary with size 1376.0 KiB\n",
      "23/11/15 13:40:20 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.9165906618905466\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc40c31",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f774c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:20 WARN DAGScheduler: Broadcasting large task binary with size 1163.3 KiB\n",
      "23/11/15 13:40:20 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:21 WARN DAGScheduler: Broadcasting large task binary with size 1163.4 KiB\n",
      "23/11/15 13:40:21 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:21 WARN DAGScheduler: Broadcasting large task binary with size 1301.8 KiB\n",
      "23/11/15 13:40:21 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:23 WARN DAGScheduler: Broadcasting large task binary with size 1530.2 KiB\n",
      "23/11/15 13:40:24 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:25 WARN DAGScheduler: Broadcasting large task binary with size 1576.1 KiB\n",
      "23/11/15 13:40:25 WARN DAGScheduler: Broadcasting large task binary with size 1627.2 KiB\n",
      "23/11/15 13:40:26 WARN DAGScheduler: Broadcasting large task binary with size 1671.1 KiB\n",
      "23/11/15 13:40:26 WARN DAGScheduler: Broadcasting large task binary with size 1718.8 KiB\n",
      "23/11/15 13:40:27 WARN DAGScheduler: Broadcasting large task binary with size 1464.0 KiB\n",
      "23/11/15 13:40:27 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13377,[0,1,2,13,...|[82.1842553767343...|[0.82184255376734...|       0.0|\n",
      "|  0.0|(13377,[0,1,5,20,...|[87.2237016666320...|[0.87223701666632...|       0.0|\n",
      "|  0.0|(13377,[0,1,7,15,...|[87.1902520524093...|[0.87190252052409...|       0.0|\n",
      "|  0.0|(13377,[0,1,17,19...|[83.4134013878543...|[0.83413401387854...|       0.0|\n",
      "|  0.0|(13377,[0,1,20,26...|[87.2237016666320...|[0.87223701666632...|       0.0|\n",
      "|  0.0|(13377,[0,1,26,34...|[84.9077787918036...|[0.84907778791803...|       0.0|\n",
      "|  0.0|(13377,[0,1,42,67...|[87.2237016666320...|[0.87223701666632...|       0.0|\n",
      "|  0.0|(13377,[0,1,70,10...|[87.2237016666320...|[0.87223701666632...|       0.0|\n",
      "|  0.0|(13377,[0,1,150,1...|[87.2237016666320...|[0.87223701666632...|       0.0|\n",
      "|  0.0|(13377,[0,1,878,1...|[87.2237016666320...|[0.87223701666632...|       0.0|\n",
      "|  0.0|(13377,[0,2,3,6,9...|[82.8271098590920...|[0.82827109859092...|       0.0|\n",
      "|  0.0|(13377,[0,2,3,8,2...|[85.4331621852241...|[0.85433162185224...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,5,1...|[86.1329373185295...|[0.86132937318529...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,5,1...|[86.1329373185295...|[0.86132937318529...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,8,1...|[84.9497121571764...|[0.84949712157176...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,25,...|[88.4488601933579...|[0.88448860193357...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,39,...|[84.1377530329420...|[0.84137753032942...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,1...|[87.4013620341385...|[0.87401362034138...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,3...|[85.7049444019587...|[0.85704944401958...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,1...|[88.1060808642305...|[0.88106080864230...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Split your preprocessed data into training and testing sets\n",
    "(training, testing) = clean_data.randomSplit([0.7, 0.3])  # Use your preprocessed dataset\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# Fit the Random Forest model on the training data\n",
    "rf_model = rf.fit(training)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "rf_predictions = rf_model.transform(testing)\n",
    "\n",
    "# Show the results\n",
    "rf_predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19872450",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89b9d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:27 WARN DAGScheduler: Broadcasting large task binary with size 1167.4 KiB\n",
      "23/11/15 13:40:27 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13377,[0,1,2,13,...|[13.8153428869027...|[0.99999899983331...|       0.0|\n",
      "|  0.0|(13377,[0,1,5,20,...|[12.1399618295425...|[0.99999465830303...|       0.0|\n",
      "|  0.0|(13377,[0,1,7,15,...|[19.3953947531997...|[0.99999999622699...|       0.0|\n",
      "|  0.0|(13377,[0,1,17,19...|[14.0644323385741...|[0.99999922035965...|       0.0|\n",
      "|  0.0|(13377,[0,1,20,26...|[15.4833586150103...|[0.99999981134743...|       0.0|\n",
      "|  0.0|(13377,[0,1,26,34...|[16.0262759335150...|[0.99999989038329...|       0.0|\n",
      "|  0.0|(13377,[0,1,42,67...|[16.5537799719103...|[0.99999993531781...|       0.0|\n",
      "|  0.0|(13377,[0,1,70,10...|[17.3795650912764...|[0.99999997167624...|       0.0|\n",
      "|  0.0|(13377,[0,1,150,1...|[15.0275025595969...|[0.99999970239622...|       0.0|\n",
      "|  0.0|(13377,[0,1,878,1...|[18.0920495750523...|[0.99999998610934...|       0.0|\n",
      "|  0.0|(13377,[0,2,3,6,9...|[42.1562154775435...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(13377,[0,2,3,8,2...|[17.8468596349481...|[0.99999998224963...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,5,1...|[117.891902686329...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(13377,[0,2,4,5,1...|[24.1976627687263...|[0.99999999996901...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,8,1...|[15.1574201501339...|[0.99999973865394...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,25,...|[19.8236296781845...|[0.99999999754129...|       0.0|\n",
      "|  0.0|(13377,[0,2,4,39,...|[14.5658880007504...|[0.99999952781192...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,1...|[15.6752339652201...|[0.99999984428432...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,3...|[10.7855444545510...|[0.99997930389730...|       0.0|\n",
      "|  0.0|(13377,[0,2,7,8,1...|[13.8279212020800...|[0.99999901233492...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:30 WARN DAGScheduler: Broadcasting large task binary with size 1272.8 KiB\n",
      "23/11/15 13:40:30 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a Logistic Regression classifier\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Fit the Logistic Regression model on the training data\n",
    "lr_model = lr.fit(training)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "lr_predictions = lr_model.transform(testing)\n",
    "\n",
    "# Show the results\n",
    "lr_predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3b984",
   "metadata": {},
   "source": [
    "## Evaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0f78cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /home/ouzema/anaconda3/lib/python3.11/site-packages (0.8.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a377084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:33 WARN DAGScheduler: Broadcasting large task binary with size 1166.4 KiB\n",
      "23/11/15 13:40:33 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:34 WARN DAGScheduler: Broadcasting large task binary with size 1143.9 KiB\n",
      "23/11/15 13:40:34 WARN DAGScheduler: Broadcasting large task binary with size 1163.3 KiB\n",
      "23/11/15 13:40:34 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:34 WARN DAGScheduler: Broadcasting large task binary with size 1163.4 KiB\n",
      "23/11/15 13:40:34 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:34 WARN DAGScheduler: Broadcasting large task binary with size 1301.8 KiB\n",
      "23/11/15 13:40:34 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1529.4 KiB\n",
      "23/11/15 13:40:37 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1577.8 KiB\n",
      "23/11/15 13:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1624.2 KiB\n",
      "23/11/15 13:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1671.1 KiB\n",
      "23/11/15 13:40:39 WARN DAGScheduler: Broadcasting large task binary with size 1725.5 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1167.4 KiB\n",
      "23/11/15 13:40:40 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:40 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1168.1 KiB\n",
      "23/11/15 13:40:42 WARN DAGScheduler: Broadcasting large task binary with size 1376.0 KiB\n",
      "23/11/15 13:40:42 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:42 WARN DAGScheduler: Broadcasting large task binary with size 1376.0 KiB\n",
      "23/11/15 13:40:42 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:42 WARN DAGScheduler: Broadcasting large task binary with size 1376.0 KiB\n",
      "23/11/15 13:40:42 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NaiveBayes\n",
      "Precision: 0.9437\n",
      "Recall: 0.9080\n",
      "F1-score: 0.9173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:43 WARN DAGScheduler: Broadcasting large task binary with size 1472.7 KiB\n",
      "23/11/15 13:40:43 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:43 WARN DAGScheduler: Broadcasting large task binary with size 1472.7 KiB\n",
      "23/11/15 13:40:43 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:43 WARN DAGScheduler: Broadcasting large task binary with size 1472.7 KiB\n",
      "23/11/15 13:40:43 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "Precision: 0.7630\n",
      "Recall: 0.8735\n",
      "F1-score: 0.8145\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:43 WARN DAGScheduler: Broadcasting large task binary with size 1277.8 KiB\n",
      "23/11/15 13:40:43 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:44 WARN DAGScheduler: Broadcasting large task binary with size 1277.8 KiB\n",
      "23/11/15 13:40:44 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Precision: 0.9832\n",
      "Recall: 0.9834\n",
      "F1-score: 0.9832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:44 WARN DAGScheduler: Broadcasting large task binary with size 1277.8 KiB\n",
      "23/11/15 13:40:44 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the preprocessed data into training and testing sets\n",
    "(training, testing) = clean_data.randomSplit([0.7, 0.3])  # Replace 'clean_data' with your actual preprocessed dataset\n",
    "\n",
    "# Create and evaluate Naive Bayes model\n",
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n",
    "nb_model = nb.fit(training)\n",
    "nb_predictions = nb_model.transform(testing)\n",
    "\n",
    "# Create and evaluate Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "rf_model = rf.fit(training)\n",
    "rf_predictions = rf_model.transform(testing)\n",
    "\n",
    "# Create and evaluate Logistic Regression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(training)\n",
    "lr_predictions = lr_model.transform(testing)\n",
    "\n",
    "# Define a function to evaluate and print metrics\n",
    "def evaluate_model(model, predictions):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate and print metrics for each model\n",
    "evaluate_model(nb, nb_predictions)\n",
    "evaluate_model(rf, rf_predictions)\n",
    "evaluate_model(lr, lr_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2b7a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:44 WARN DAGScheduler: Broadcasting large task binary with size 1376.0 KiB\n",
      "23/11/15 13:40:44 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:45 WARN DAGScheduler: Broadcasting large task binary with size 1376.0 KiB\n",
      "23/11/15 13:40:45 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:45 WARN DAGScheduler: Broadcasting large task binary with size 1376.0 KiB\n",
      "23/11/15 13:40:45 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+----------+------------+\n",
      "| Model      |   Precision |   Recall |   F1-Score |\n",
      "+============+=============+==========+============+\n",
      "| NaiveBayes |      0.9437 |    0.908 |     0.9173 |\n",
      "+------------+-------------+----------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:45 WARN DAGScheduler: Broadcasting large task binary with size 1472.7 KiB\n",
      "23/11/15 13:40:45 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:45 WARN DAGScheduler: Broadcasting large task binary with size 1472.7 KiB\n",
      "23/11/15 13:40:45 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:46 WARN DAGScheduler: Broadcasting large task binary with size 1472.7 KiB\n",
      "23/11/15 13:40:46 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------+----------+------------+\n",
      "| Model                  |   Precision |   Recall |   F1-Score |\n",
      "+========================+=============+==========+============+\n",
      "| RandomForestClassifier |       0.763 |   0.8735 |     0.8145 |\n",
      "+------------------------+-------------+----------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:46 WARN DAGScheduler: Broadcasting large task binary with size 1277.8 KiB\n",
      "23/11/15 13:40:46 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n",
      "23/11/15 13:40:46 WARN DAGScheduler: Broadcasting large task binary with size 1277.8 KiB\n",
      "23/11/15 13:40:46 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+------------+\n",
      "| Model              |   Precision |   Recall |   F1-Score |\n",
      "+====================+=============+==========+============+\n",
      "| LogisticRegression |      0.9832 |   0.9834 |     0.9832 |\n",
      "+--------------------+-------------+----------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:40:46 WARN DAGScheduler: Broadcasting large task binary with size 1277.8 KiB\n",
      "23/11/15 13:40:46 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 5, schema size: 2\n",
      "CSV file: file:///home/ouzema/Big%20Data/spam.csv\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def evaluate_model_table(model, predictions):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    table = [\n",
    "        [\"Model\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
    "        [model.__class__.__name__, f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\"]\n",
    "    ]\n",
    "\n",
    "    print(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "\n",
    "# Evaluate and print metrics for each model in a table\n",
    "evaluate_model_table(nb, nb_predictions)\n",
    "evaluate_model_table(rf, rf_predictions)\n",
    "evaluate_model_table(lr, lr_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1430e",
   "metadata": {},
   "source": [
    "#### Enregistrement du modèle  ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc058fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save your model as a directory of files\n",
    "#lr_model.save(\"lr_model\")\n",
    "#rf_model.save(\"rf_model\")\n",
    "#nb_model.save(\"nb_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f9b83",
   "metadata": {},
   "source": [
    "### Utilisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb951f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = spark.read.csv(\"spam2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ecc5ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|          COMMENT_ID|              AUTHOR|                DATE|             CONTENT|CLASS|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|z13lgffb5w3ddx1ul...|          dharma pal|2015-05-29 02:30:...|          Nice song﻿|    0|\n",
      "|z123dbgb0mqjfxbtz...|       Tiza Arellano|2015-05-29 00:14:...|       I love song ﻿|    0|\n",
      "|z12quxxp2vutflkxv...|Prìñçeśś Âliś Łøv...|2015-05-28 21:00:...|       I love song ﻿|    0|\n",
      "|z12icv3ysqvlwth2c...|       Eric Gonzalez|2015-05-28 20:47:...|860,000,000 lets ...|    0|\n",
      "|z133stly3kete3tly...|       Analena López|2015-05-28 17:08:...|shakira is best f...|    0|\n",
      "|z12myn4rltf4ejddv...| jehoiada wellington|2015-05-28 17:06:...|The best world cu...|    0|\n",
      "|z135vzqy1yrjhluew...|    Kara Cuthbertson|2015-05-28 15:46:...|             I love﻿|    0|\n",
      "|z12uujnj2sifvzvav...|       Sudheer Yadav|2015-05-28 10:28:...|SEE SOME MORE SON...|    1|\n",
      "|z13lvh1qnma4d15sy...|           Alex John|2015-05-28 07:44:...|           Awesome ﻿|    0|\n",
      "|z135hlk5grfwjhmym...|     Nirab Valobasha|2015-05-27 21:31:...|   I like shakira..﻿|    0|\n",
      "|z121cvvqftuvuz1ld...|     Chelsea Andrews|2015-05-27 20:43:...|Shakira - Waka Wa...|    0|\n",
      "|z12uf5gatnf1dflws...|          Kevin Sosa|2015-05-27 20:04:...|Why so many disli...|    0|\n",
      "|z13pihfbwlv5hv4ap...|     OldSchool Music|2015-05-27 19:00:...|I don&#39;t think...|    0|\n",
      "|z13utfk5rny3yzqtj...|      Stephen Lister|2015-05-27 16:04:...|          Love song﻿|    0|\n",
      "|z13usjdoivinwzsoy...|    Karolína Hlavatá|2015-05-27 14:41:...|          wery good﻿|    0|\n",
      "|z12bhf4rzpjsvjmcw...|           DubCedSky|2015-05-27 13:51:...|Every time I hear...|    0|\n",
      "|z132i1cj3t2cedajp...|       akita hachiko|2015-05-27 13:07:...|Whose watching th...|    0|\n",
      "|z12aw1ah2m2vh30tb...|        Benjy Growls|2015-05-27 09:27:...|I love this song ...|    0|\n",
      "|z12cydggrzyesrklw...|      monkey moments|2015-05-27 09:24:...|i love this song ...|    0|\n",
      "|z12gddhblwz3cf3wc...|Dr.geetanjali sharma|2015-05-27 09:14:...|      Waka best one﻿|    0|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e786ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             CONTENT|CLASS|\n",
      "+--------------------+-----+\n",
      "|          Nice song﻿|    0|\n",
      "|       I love song ﻿|    0|\n",
      "|       I love song ﻿|    0|\n",
      "|860,000,000 lets ...|    0|\n",
      "|shakira is best f...|    0|\n",
      "|The best world cu...|    0|\n",
      "|             I love﻿|    0|\n",
      "|SEE SOME MORE SON...|    1|\n",
      "|           Awesome ﻿|    0|\n",
      "|   I like shakira..﻿|    0|\n",
      "|Shakira - Waka Wa...|    0|\n",
      "|Why so many disli...|    0|\n",
      "|I don&#39;t think...|    0|\n",
      "|          Love song﻿|    0|\n",
      "|          wery good﻿|    0|\n",
      "|Every time I hear...|    0|\n",
      "|Whose watching th...|    0|\n",
      "|I love this song ...|    0|\n",
      "|i love this song ...|    0|\n",
      "|      Waka best one﻿|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dataset = new_dataset.select(\"CONTENT\", \"CLASS\")\n",
    "new_dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81805b9",
   "metadata": {},
   "source": [
    "### Traitement de la nouvelle dataset  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a987bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|             CONTENT|CLASS|length|\n",
      "+--------------------+-----+------+\n",
      "|          Nice song﻿|    0|    10|\n",
      "|       I love song ﻿|    0|    13|\n",
      "|       I love song ﻿|    0|    13|\n",
      "|860,000,000 lets ...|    0|    86|\n",
      "|shakira is best f...|    0|    29|\n",
      "|The best world cu...|    0|    33|\n",
      "|             I love﻿|    0|     7|\n",
      "|SEE SOME MORE SON...|    1|    60|\n",
      "|           Awesome ﻿|    0|     9|\n",
      "|   I like shakira..﻿|    0|    17|\n",
      "|Shakira - Waka Wa...|    0|    56|\n",
      "|Why so many disli...|    0|    34|\n",
      "|I don&#39;t think...|    0|    47|\n",
      "|          Love song﻿|    0|    10|\n",
      "|          wery good﻿|    0|    10|\n",
      "|Every time I hear...|    0|    89|\n",
      "|Whose watching th...|    0|    40|\n",
      "|I love this song ...|    0|    57|\n",
      "|i love this song ...|    0|    34|\n",
      "|      Waka best one﻿|    0|    14|\n",
      "+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "new_dataset = new_dataset.withColumn('length',length(new_dataset['CONTENT']))\n",
    "new_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "118788e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------+\n",
      "|CLASS|avg(CLASS)|       avg(length)|\n",
      "+-----+----------+------------------+\n",
      "|    1|       1.0|191.17919075144508|\n",
      "|    0|       0.0|31.321428571428573|\n",
      "+-----+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dataset.groupby('CLASS').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27f05251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|             CONTENT|CLASS|length|\n",
      "+--------------------+-----+------+\n",
      "|          Nice song﻿|    0|    10|\n",
      "|       I love song ﻿|    0|    13|\n",
      "|       I love song ﻿|    0|    13|\n",
      "|860,000,000 lets ...|    0|    86|\n",
      "|shakira is best f...|    0|    29|\n",
      "|The best world cu...|    0|    33|\n",
      "|             I love﻿|    0|     7|\n",
      "|SEE SOME MORE SON...|    1|    60|\n",
      "|           Awesome ﻿|    0|     9|\n",
      "|   I like shakira..﻿|    0|    17|\n",
      "|Shakira - Waka Wa...|    0|    56|\n",
      "|Why so many disli...|    0|    34|\n",
      "|I don&#39;t think...|    0|    47|\n",
      "|          Love song﻿|    0|    10|\n",
      "|          wery good﻿|    0|    10|\n",
      "|Every time I hear...|    0|    89|\n",
      "|Whose watching th...|    0|    40|\n",
      "|I love this song ...|    0|    57|\n",
      "|i love this song ...|    0|    34|\n",
      "|      Waka best one﻿|    0|    14|\n",
      "+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dataset = new_dataset.dropna(subset=[\"CLASS\"])\n",
    "new_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8829381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the lit() function\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Convert the values 0 and 1 to strings\n",
    "new_dataset = new_dataset.withColumn(\"CLASS\", when(new_dataset[\"CLASS\"] == 0, lit(\"ham\")).otherwise(lit(\"spam\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "171e4670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                text|class|length|\n",
      "+--------------------+-----+------+\n",
      "|          Nice song﻿|  ham|    10|\n",
      "|       I love song ﻿|  ham|    13|\n",
      "|       I love song ﻿|  ham|    13|\n",
      "|860,000,000 lets ...|  ham|    86|\n",
      "|shakira is best f...|  ham|    29|\n",
      "|The best world cu...|  ham|    33|\n",
      "|             I love﻿|  ham|     7|\n",
      "|SEE SOME MORE SON...| spam|    60|\n",
      "|           Awesome ﻿|  ham|     9|\n",
      "|   I like shakira..﻿|  ham|    17|\n",
      "|Shakira - Waka Wa...|  ham|    56|\n",
      "|Why so many disli...|  ham|    34|\n",
      "|I don&#39;t think...|  ham|    47|\n",
      "|          Love song﻿|  ham|    10|\n",
      "|          wery good﻿|  ham|    10|\n",
      "|Every time I hear...|  ham|    89|\n",
      "|Whose watching th...|  ham|    40|\n",
      "|I love this song ...|  ham|    57|\n",
      "|i love this song ...|  ham|    34|\n",
      "|      Waka best one﻿|  ham|    14|\n",
      "+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the CONTENT column to text\n",
    "new_dataset = new_dataset.withColumnRenamed(\"CONTENT\", \"text\")\n",
    "# Rename the CLASS column to class\n",
    "new_dataset = new_dataset.withColumnRenamed(\"CLASS\", \"class\")\n",
    "# Show the updated dataframe\n",
    "new_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1ebd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72840e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n",
    "cleaner_new = data_prep_pipe.fit(new_dataset)\n",
    "clean_new_dataset = cleaner.transform(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6918a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LogisticRegressionModel class from the pyspark.ml.classification module\n",
    "from pyspark.ml.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40c3c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.ml.classification module\n",
    "import pyspark.ml.classification as cl\n",
    "# Use the dot notation to access the LogisticRegressionModel class\n",
    "lr_model = cl.LogisticRegressionModel.load(\"lr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a133da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the new dataset\n",
    "predictions = lr_model.transform(clean_new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4f69ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:42:34 WARN DAGScheduler: Broadcasting large task binary with size 1189.8 KiB\n",
      "23/11/15 13:42:34 WARN DAGScheduler: Broadcasting large task binary with size 1149.2 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1186.7 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1186.7 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1325.2 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1420.9 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1450.2 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1477.4 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1500.3 KiB\n",
      "23/11/15 13:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1523.7 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1190.8 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:36 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1191.5 KiB\n",
      "23/11/15 13:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1399.3 KiB\n",
      "23/11/15 13:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1399.3 KiB\n",
      "23/11/15 13:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1399.3 KiB\n",
      "23/11/15 13:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1467.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NaiveBayes\n",
      "Precision: 0.7780\n",
      "Recall: 0.6080\n",
      "F1-score: 0.5307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1467.1 KiB\n",
      "23/11/15 13:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1467.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "Precision: 0.7778\n",
      "Recall: 0.5920\n",
      "F1-score: 0.5170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1203.4 KiB\n",
      "23/11/15 13:42:39 WARN DAGScheduler: Broadcasting large task binary with size 1203.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Precision: 0.8968\n",
      "Recall: 0.8960\n",
      "F1-score: 0.8959\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:42:39 WARN DAGScheduler: Broadcasting large task binary with size 1203.4 KiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the preprocessed data into training and testing sets\n",
    "(training, testing) = clean_new_dataset.randomSplit([0.7, 0.3])  \n",
    "\n",
    "# Create and evaluate Naive Bayes model\n",
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n",
    "nb_model = nb.fit(training)\n",
    "nb_predictions = nb_model.transform(testing)\n",
    "\n",
    "# Create and evaluate Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "rf_model = rf.fit(training)\n",
    "rf_predictions = rf_model.transform(testing)\n",
    "\n",
    "# Create and evaluate Logistic Regression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(training)\n",
    "lr_predictions = lr_model.transform(testing)\n",
    "\n",
    "# Define a function to evaluate and print metrics\n",
    "def evaluate_model(model, predictions):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate and print metrics for each model\n",
    "evaluate_model(nb, nb_predictions)\n",
    "evaluate_model(rf, rf_predictions)\n",
    "evaluate_model(lr, lr_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912cfe41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
